{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7564,"status":"ok","timestamp":1739265024189,"user":{"displayName":"Ashik Ai","userId":"09322664290714215712"},"user_tz":-330},"id":"OUObCeGXjQcE"},"outputs":[],"source":["import nltk  # Natural Language Toolkit for text processing\n","import string  # For handling punctuation\n","import random  # For generating random responses\n","import numpy as np  # For numerical operations\n","from nltk.stem import WordNetLemmatizer  # For word lemmatization\n","from sklearn.feature_extraction.text import TfidfVectorizer  # For text vectorization\n","from sklearn.metrics.pairwise import cosine_similarity  # For finding similarity\n","import sys  # For handling system exit in Colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":718,"status":"ok","timestamp":1739265063862,"user":{"displayName":"Ashik Ai","userId":"09322664290714215712"},"user_tz":-330},"id":"Zfx4djsVhUev","outputId":"6f57a6d7-9589-4f8d-c992-55c1553a7eb1"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Download necessary NLTK data files\n","nltk.download('punkt')  # Tokenizer data\n","nltk.download('wordnet')  # WordNet lemmatizer data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TQSgUdDEhevN"},"outputs":[{"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 3) (\u003cipython-input-3-4381196d00be\u003e, line 3)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-3-4381196d00be\u003e\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    with open('/content/drive/MyDrive/PROJECT/dialogs.txt, 'r', errors='ignore') as f:\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"]}],"source":["# Load and preprocess dataset\n","try:\n","    with open('/content/drive/MyDrive/ai tryouts/dialogs.txt', 'r', errors='ignore') as f:\n","        raw = f.read().lower()  # Read file and convert to lowercase\n","        if not raw.strip():\n","            print(\"ERROR: dialogs.txt is empty. Please add conversation data.\")\n","            sys.exit()\n","except FileNotFoundError:\n","    print(\"ERROR: dialogs.txt not found. Check the file path and try again.\")\n","    sys.exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HlDCKfeQhymQ"},"outputs":[],"source":["# Split dataset into question-answer pairs\n","qa_pairs = [line.split('\\t') for line in raw.split('\\n') if '\\t' in line]\n","question_answer_dict = {q.strip(): a.strip() for q, a in qa_pairs}\n","sent_tokens = list(question_answer_dict.keys())  # Store only questions\n","\n","if len(sent_tokens) \u003c 2:\n","    print(\"ERROR: Not enough data to train the chatbot. Add more conversations to dialogs.txt.\")\n","    sys.exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrfFJFpriEMX"},"outputs":[],"source":["# Initialize lemmatizer\n","lemmer = WordNetLemmatizer()\n","\n","def LemTokens(tokens):\n","    \"\"\"Lemmatize a list of tokens\"\"\"\n","    return [lemmer.lemmatize(token) for token in tokens]\n","\n","def LemNormalize(text):\n","    \"\"\"Normalize text by removing punctuation and lemmatizing words\"\"\"\n","    remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n","    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4O-q8fSkSKS"},"outputs":[],"source":["def response(user_response):\n","    \"\"\"Generate a chatbot response based on dataset matching\"\"\"\n","    user_response = user_response.strip()\n","    if user_response in question_answer_dict:\n","        return question_answer_dict[user_response]  # Direct match from dataset\n","\n","    # If no direct match, use TF-IDF similarity\n","    sent_tokens.append(user_response)  # Temporarily add user input\n","    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n","    tfidf = TfidfVec.fit_transform(sent_tokens)\n","    vals = cosine_similarity(tfidf[-1], tfidf)  # Compute similarity with all sentences\n","    idx = vals.argsort()[0][-2]\n","    flat = vals.flatten()\n","    flat.sort()\n","    req_tfidf = flat[-2]\n","    sent_tokens.pop()  # Remove user input after processing\n","\n","    if req_tfidf == 0:\n","        return \"I'm sorry! I don't understand you. Can you rephrase?\"\n","    else:\n","        return question_answer_dict.get(sent_tokens[idx], \"I'm sorry! I don't understand you.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25751,"status":"ok","timestamp":1739236555479,"user":{"displayName":"Ashik Ai","userId":"09322664290714215712"},"user_tz":-330},"id":"RUqYwOIciq0s","outputId":"8c8ce18f-936e-4939-de6b-fd4c5834032c"},"outputs":[{"name":"stdout","output_type":"stream","text":["BOT: My name is Stark. Let's chat! Type 'bye' to exit.\n","You: HI\n","BOT: I'm glad you're talking to me!\n","You: HOW ARE YOU\n","BOT: I'm just a bot, but I'm feeling awesome!\n","You: BYE\n","BOT: Goodbye! Take care.\n","BOT: Chat session ended.\n"]}],"source":["# Chatbot loop\n","print(\"BOT: My name is Stark. Let's chat! Type 'bye' to exit.\")\n","while True:\n","    try:\n","        user_response = input(\"You: \").strip().lower()  # Get user input\n","        if not user_response:\n","            print(\"BOT: Please enter some text to continue the conversation.\")\n","            continue\n","        if user_response == 'bye':\n","            print(\"BOT: Goodbye! Take care.\")\n","            print(\"BOT: Chat session ended.\")\n","            break\n","        elif user_response in ('thanks', 'thank you'):\n","            print(\"BOT: You're welcome!\")\n","            print(\"BOT: Chat session ended.\")\n","            break\n","        else:\n","            bot_response = greet(user_response)  # Check if greeting\n","            if bot_response:\n","                print(\"BOT:\", bot_response)\n","            else:\n","                print(\"BOT:\", response(user_response))  # Generate response using dataset or TF-IDF\n","    except KeyboardInterrupt:\n","        print(\"\\nBOT: Chat session interrupted. Goodbye!\")\n","        break\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNi99aVIcct/Sc0/f1BHr7e","mount_file_id":"1uErpnyZmqv0qobMzlQB5ZsuidTRJlJK8","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}